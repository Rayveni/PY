{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from mlcompetitions import *\n",
    "import random\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "#rom IPython.core.interactiveshell import InteractiveShell\n",
    "#nteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml=ML_competitions(r'project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "macro_cols = [\"balance_trade\", \"balance_trade_growth\", \"usdrub\", \"average_provision_of_build_contract\",\n",
    "\"micex_rgbi_tr\", \"micex_cbi_tr\", \"deposits_rate\", \"mortgage_value\", \"mortgage_rate\",\n",
    "\"income_per_cap\", \"rent_price_4+room_bus\", \"museum_visitis_per_100_cap\", \"apartment_build\"]\n",
    "add_macro_cols=['average_provision_of_build_contract_moscow',\n",
    " 'cpi',\n",
    " 'deposits_growth',\n",
    " 'gdp_quart_growth',\n",
    " 'mortgage_growth',\n",
    " 'net_capital_export',\n",
    " 'ppi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path=r'data'\n",
    "df_train =pd.read_csv(os.path.join(path,'train.csv'),sep=',',parse_dates=['timestamp'])\n",
    "df_test = pd.read_csv(os.path.join(path,'test.csv'),sep=',',parse_dates=['timestamp'])\n",
    "df_macro = pd.read_csv(os.path.join(path,'macro.csv'),sep=',',parse_dates=['timestamp'], usecols=['timestamp'] + macro_cols+add_macro_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml.load_data(df_train.drop('price_doc',axis=1),df_train.price_doc.values.ravel(),df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ylog_train_all = np.log1p(df_train['price_doc'].values)\n",
    "id_test = df_test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ecology_params=['park_km','green_zone_km','industrial_km','water_treatment_km','cemetery_km',\n",
    " 'incineration_km','railroad_station_avto_km','public_transport_station_km','water_km',\n",
    " 'water_1line','mkad_km','big_road1_1line','big_road2_km','railroad_km', 'oil_chemistry_km',\n",
    " 'nuclear_reactor_km','radiation_km','power_transmission_line_km','thermal_power_plant_km','green_part_500']\n",
    "\n",
    "\n",
    "house_loc_params=['ID_metro','sub_area',\n",
    " 'area_m',\n",
    " 'metro_min_avto',\n",
    " 'metro_km_avto',\n",
    " 'metro_min_walk',\n",
    " 'metro_km_walk',\n",
    " 'kindergarten_km',\n",
    " 'school_km',\n",
    " 'park_km',\n",
    " 'green_zone_km',\n",
    " 'industrial_km',\n",
    " 'water_treatment_km',\n",
    " 'cemetery_km',\n",
    " 'incineration_km',\n",
    " 'railroad_station_walk_km',\n",
    " 'railroad_station_walk_min',\n",
    " 'ID_railroad_station_walk']\n",
    "\n",
    "\n",
    "\n",
    "def most_common(arr):\n",
    "    t=arr.dropna()\n",
    "    if len(t)>0:\n",
    "        return Counter(list(arr)).most_common(1)[0][0]\n",
    "    else: return np.nan\n",
    "\n",
    "def eco_forest(df,ecology_params):\n",
    "    eco_df=df[ecology_params+['ecology']].drop_duplicates()\n",
    "    eco_df=eco_df[eco_df.ecology!='no data']\n",
    "    eco_df.dropna(inplace=True,axis=1)\n",
    "    lbl=LabelEncoder()\n",
    "    lbl.fit(eco_df.ecology)\n",
    "    y_eco=lbl.transform(eco_df.ecology)\n",
    "    dummy_df=pd.get_dummies(eco_df[['big_road1_1line','water_1line']])\n",
    "    eco_df.drop(['ecology','big_road1_1line','water_1line'],axis=1,inplace=True)\n",
    "    rf=RandomForestClassifier(n_estimators=2000)\n",
    "  \n",
    "    scaler=StandardScaler().fit(eco_df)\n",
    "\n",
    "    rf.fit(np.hstack([dummy_df.values,scaler.transform(eco_df)]),y_eco)\n",
    "    return rf,lbl,scaler,eco_df.columns\n",
    "\n",
    "def ecology_replace(x):\n",
    "    x_uniq=x.unique()\n",
    "    if len(x_uniq)>1:\n",
    "        x=[el for el in x if el!='no data']\n",
    "        res=max(set(x), key=x.count)\n",
    "    else:\n",
    "        res=x_uniq[0]\n",
    "    return res\n",
    "    \n",
    "replace_arr=lambda column,data,dict_df:data[[column,'id_house']].apply(lambda row:dict_df.ix[row[1]][column],axis=1)    \n",
    "replace_arr_with_nan=lambda column,data,dict_df:data[[column,'id_house']].apply(lambda row:dict_df.ix[row[1]][column]\n",
    "                                                       if np.isnan(row[0]) else row[0] ,axis=1)     \n",
    "    \n",
    "def prerpocess_house_params(data):\n",
    "    df=data.copy(deep=True)\n",
    "    df['full_sq']=df[['full_sq','life_sq']].apply(lambda row:max(row[0],row[1]),axis=1)\n",
    "    unique_houses=df[house_loc_params].drop_duplicates().reset_index(drop=True)\n",
    "    unique_houses['id_house']=range(unique_houses.shape[0])\n",
    "    df=pd.merge(df,unique_houses ,on =house_loc_params,how='left')\n",
    "    unique_houses_groped=df.groupby(['id_house']).agg({\n",
    "\n",
    "    'max_floor' : most_common, #np.median, \n",
    "    'material':most_common ,#np.median,\n",
    "    'build_year':most_common, #np.median,\n",
    "    'state':most_common, #np.median,\n",
    "    'ecology':ecology_replace\n",
    "    })  \n",
    "    #df['max_floor']=df[['max_floor','id_house']].apply(lambda row:unique_houses_groped.ix[row[1]]['max_floor']\n",
    "    #                                                  if np.isnan(row[0]) else row[0] ,axis=1)\n",
    "    df['max_floor']=replace_arr('max_floor',df,unique_houses_groped)\n",
    "    df['material']=replace_arr('material',df,unique_houses_groped)\n",
    "    df['build_year']=replace_arr('build_year',df,unique_houses_groped)\n",
    "    df['state']=replace_arr_with_nan('state',df,unique_houses_groped)\n",
    "    #df['ecology']=replace_arr('ecology',df,unique_houses_groped)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def squar_fix_outliner(data):\n",
    "    data['full_sq']=data['full_sq'].apply(lambda x:x/100 if x>4000 else x)\n",
    "    data['life_sq']=data['life_sq'].apply(lambda x:x/100 if x>4000 else x)\n",
    "    return data\n",
    "\n",
    "def closest_in_list(myNumber,myList):\n",
    "    return min(myList, key=lambda x:abs(x-myNumber))\n",
    "\n",
    "\n",
    "def fix_square(data):\n",
    "    manual_sq={23231:74,33974:60,36824:64}\n",
    "    df=data.copy(deep=True)\n",
    "    df['price_sq']=df_train.price_doc/df.full_sq\n",
    "    agg_price_house=df.groupby(['id_house']).agg({\n",
    "            'price_sq':np.median,'full_sq':lambda x:list(x)\n",
    "        })\n",
    "    df['estimated_sq']=df_train.price_doc/   df.id_house.apply(lambda x:agg_price_house.ix[x].price_sq)\n",
    "   \n",
    "    df['full_sq']=df[['estimated_sq','id_house','full_sq']].apply(lambda row:closest_in_list(row[0],agg_price_house.ix[row[1]].full_sq) if row[2]<10 else row[2],axis=1)\n",
    "    df['full_sq']=df[['full_sq','id']].apply(lambda row:manual_sq[row[1]] if row[1] in list(manual_sq.keys())else row[0],axis=1)\n",
    "    return df.drop(['price_sq','estimated_sq'],axis=1)\n",
    "\n",
    "\n",
    "def correct_life_square(full_sq,life_sq,dict_val,life_prop,min_th,max_th,th):\n",
    "    if pd.isnull(life_sq):\n",
    "        return dict_val*full_sq\n",
    "    elif life_prop> min_th*(1-th) and life_prop<max_th*(1+th):\n",
    "        return life_sq\n",
    "    else:\n",
    "        return dict_val*full_sq\n",
    "    \n",
    "\n",
    "\n",
    "def life_fix_missing(data,th=0.25):\n",
    "\n",
    "    df=data.copy(deep=True)\n",
    "    df['life_prop']=df.life_sq/df.full_sq\n",
    "    life_house_groped=df.groupby(['id_house','sub_area']).agg({'life_prop':np.median,}).reset_index()\n",
    "    life_area_groped=df.groupby(['sub_area']).agg({'life_prop':np.median}).reset_index()\n",
    "\n",
    "    life_house_groped=pd.merge(life_house_groped,life_area_groped ,on='sub_area',how='left')\n",
    "    life_house_groped=life_house_groped.ix[:,-2:].mean(axis=1).to_dict()\n",
    "    min_th=min(list(life_house_groped.values()))\n",
    "    max_th=max(list(life_house_groped.values()))\n",
    "    df['life_sq']=df[['life_sq','full_sq',\n",
    "                      'id_house','life_prop']].apply(lambda row:\n",
    "                                                     correct_life_square(row[1],\n",
    "                                                                         row[0],\n",
    "                                                                         life_house_groped[row[2]],row[3],\n",
    "                                                                         min_th,max_th,th),axis=1)\n",
    "    df.drop('life_prop',axis=1,inplace=True)\n",
    "    return df\n",
    "def process_build_year(data):\n",
    "    \n",
    "    data['build_year']=data.build_year.apply(lambda x: -10000 if x<1800 or np.isnan(x) else x)\n",
    "    return data\n",
    "def transform_knn(data):\n",
    "    df=data.copy(deep=True)\n",
    "    df.fillna('nan')\n",
    "    scaler=StandardScaler()\n",
    "    df['full_sq']=scaler.fit_transform(df.full_sq)\n",
    "    cosine_cat_columns=['id_house','build_year','sub_area','product_type']\n",
    "    df_cat=pd.get_dummies(df[cosine_cat_columns])\n",
    "    df_cat['full_sq']=df['full_sq']\n",
    "    return df_cat\n",
    "def fix_kitch_outliners(data,up_th=1.5,low_th=0.7):\n",
    "    df=data.copy(deep=True)\n",
    "    df['build_year']=df[['build_year','id']].apply(lambda row:2014 if row[1]==21418 else row[0],axis=1)\n",
    "    index=df[(df.kitch_sq>5) & (df.kitch_sq<df.full_sq)].index\n",
    "    target=df.ix[index].kitch_sq.values\n",
    "    knn_df=transform_knn(df)\n",
    "    #knn=KNeighborsRegressor()\n",
    "    #knn.fit(knn_df.ix[index],target)\n",
    "    rf=RandomForestRegressor(n_estimators=1000)\n",
    "    rf.fit(knn_df.ix[index],target)\n",
    "    df['pred_kitch']=rf.predict(knn_df)\n",
    "    df['kitch_sq']=df.kitch_sq.fillna(0)\n",
    "    df['delta_kitch']=df.kitch_sq/df.pred_kitch\n",
    "    df['kitch_sq']=df[['kitch_sq','pred_kitch','delta_kitch']].apply(lambda row: row[0] if row[2] >low_th and row[2]<up_th else row[1],axis=1)\n",
    "    return df.drop(['delta_kitch','pred_kitch'],axis=1)\n",
    "\n",
    "def life_sq_fix(data):\n",
    "    data['life_sq']=data[['full_sq','life_sq','kitch_sq']].apply(lambda row:\n",
    "                                                                 row[1] if row[1]<=row[0]-row[2] else row[0]-row[2],axis=1)\n",
    "    return data\n",
    "\n",
    "def fix_max_floor(data):\n",
    "    err_floor=pd.concat([data[data.max_floor<data.floor],data[pd.isnull(data.max_floor)]],axis=0).drop_duplicates()\n",
    "    err_floor=pd.DataFrame(err_floor.pivot_table(index='id_house',values='floor',aggfunc=np.max)).reset_index()\n",
    "    err_floor.columns=['id_house','err_max_floor_flg']\n",
    "    data=pd.merge(data,err_floor,on='id_house',how='left')\n",
    "    data['max_floor']=data[['max_floor','err_max_floor_flg']].apply(lambda row:max(row[1],row[0]),axis=1)\n",
    "    data['c']=(data['err_max_floor_flg']/data['err_max_floor_flg']).fillna(0)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:58: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:102: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:127: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:155: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:160: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7662, 323)\n",
      "Wall time: 18min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def f_transform(X):\n",
    "    \n",
    "    X = pd.merge(X, df_macro, on='timestamp', how='left')\n",
    "    # Add month-year\n",
    "    month_year = (X.timestamp.dt.month + X.timestamp.dt.year * 100)\n",
    "    month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "    X['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "    \n",
    "    # Add week-year count\n",
    "    week_year = (X.timestamp.dt.weekofyear + X.timestamp.dt.year * 100)\n",
    "    week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "    X['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "    \n",
    "    # Add month and day-of-week\n",
    "    X['month'] = X.timestamp.dt.month\n",
    "    X['dow'] = X.timestamp.dt.dayofweek\n",
    "    \n",
    "\n",
    "    #####################\n",
    "    \n",
    "    \n",
    "    \n",
    "    X=prerpocess_house_params(X)\n",
    "    \n",
    "    X=squar_fix_outliner(X)\n",
    "    X=fix_square(X)\n",
    "    ########\n",
    "    X=life_fix_missing(X)\n",
    "    X=process_build_year(X)\n",
    "    X=fix_kitch_outliners(X)\n",
    "    X=life_sq_fix(X)\n",
    "    X=fix_max_floor(X)\n",
    " \n",
    "    ################ \n",
    "    # Other feature engineering\n",
    "    X['rel_floor'] = X['floor'] / X['max_floor'].astype(float)\n",
    "    X['rel_kitch_sq'] = X['kitch_sq'] / X['full_sq'].astype(float)\n",
    "    X[\"ratio_life_sq_full_sq\"] = X[\"life_sq\"] / X[\"full_sq\"]\n",
    "    # ratio of kitchen area to full area #\n",
    "    #building year\n",
    "    X['year']=X[\"timestamp\"].dt.year\n",
    "    X[\"age_of_building\"] = X[\"build_year\"] - X[\"year\"]\n",
    "\n",
    "    #last fist floor\n",
    "    X['fist_floor']=X.floor.apply(lambda x:1 if x==1 else 0)\n",
    "    X['last_floor']=X[['floor','max_floor']].apply(lambda x:1 if (x[1]-x[0])<1 else 0,axis=1)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    X.drop(['timestamp'], axis=1, inplace=True)\n",
    "    X.drop(['id'], axis=1, inplace=True)\n",
    "    # Deal with categorical values\n",
    "    df_numeric = X.select_dtypes(exclude=['object'])\n",
    "    df_obj = X.select_dtypes(include=['object']).copy()\n",
    "\n",
    "    for c in df_obj:\n",
    "        df_obj[c] = pd.factorize(X[c])[0]\n",
    "\n",
    "    X = pd.concat([df_numeric, df_obj], axis=1)\n",
    "    \n",
    "    \n",
    "    return X\n",
    "a=ml.transform(f_transform)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=a[0].copy(deep=True)\n",
    "X['price_meter']=a[1]/X.full_sq\n",
    "median_price=np.median(X.price_meter.dropna())\n",
    "X['temp_usd_compare']=X.price_meter*X.usdrub/median_price\n",
    "X['price']=a[1]\n",
    "X['price']=X[['price','full_sq',\n",
    "              'temp_usd_compare','usdrub']].apply(lambda row:row[3]*row[1] if row[2]<3 else row[0],axis=1)\n",
    "\n",
    "corrected_price=X['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_all shape is (30471, 323)\n",
      "X_train shape is (24377, 323)\n",
      "y_train shape is (24377,)\n",
      "X_val shape is (6094, 323)\n",
      "y_val shape is (6094,)\n",
      "X_test shape is (7662, 323)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy values\n",
    "\n",
    "num_train=df_train.shape[0]\n",
    "# Create a validation set, with last 20% of data\n",
    "num_val = int(num_train * 0.2)\n",
    "\n",
    "X_train_all = a[0]\n",
    "X_train = X_train_all[:num_train-num_val]\n",
    "X_val = X_train_all[num_train-num_val:num_train]\n",
    "#ylog_train_all =np.log1p( a[1])\n",
    "ylog_train_all=a[1]\n",
    "#ylog_train_all =np.log1p( corrected_price)\n",
    "\n",
    "ylog_train = ylog_train_all[:-num_val]\n",
    "ylog_val = ylog_train_all[-num_val:]\n",
    "\n",
    "X_test = a[2]\n",
    "\n",
    "df_columns = a[0].columns\n",
    "\n",
    "print('X_train_all shape is', X_train_all.shape)\n",
    "print('X_train shape is', X_train.shape)\n",
    "print('y_train shape is', ylog_train.shape)\n",
    "print('X_val shape is', X_val.shape)\n",
    "print('y_val shape is', ylog_val.shape)\n",
    "print('X_test shape is', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importance=pd.read_csv('importance.csv')\n",
    "#importance_features=list(importance[importance.importance>=32].Feature)\n",
    "\n",
    "importance_features=['full_sq',\n",
    " 'floor',\n",
    " 'life_sq',\n",
    " 'micex_cbi_tr',\n",
    " 'build_year',\n",
    " 'usdrub',\n",
    " 'kindergarten_km',\n",
    " 'max_floor',\n",
    " 'green_zone_km',\n",
    " 'micex_rgbi_tr',\n",
    " 'state',\n",
    " 'metro_km_avto',\n",
    " 'additional_education_km',\n",
    " 'public_healthcare_km',\n",
    " 'big_market_km',\n",
    " 'mortgage_rate',\n",
    " 'mosque_km',\n",
    " 'railroad_km',\n",
    " 'metro_min_avto',\n",
    " 'public_transport_station_km',\n",
    " 'area_m',\n",
    " 'bus_terminal_avto_km',\n",
    " 'water_treatment_km',\n",
    " 'big_road2_km',\n",
    " 'preschool_km',\n",
    " 'thermal_power_plant_km',\n",
    " 'hospice_morgue_km',\n",
    " 'workplaces_km',\n",
    " 'week_year_cnt',\n",
    " 'park_km',\n",
    " 'rel_floor',\n",
    " 'ts_km',\n",
    " 'prom_part_5000',\n",
    " 'market_shop_km',\n",
    " 'balance_trade',\n",
    " 'nuclear_reactor_km',\n",
    " 'industrial_km',\n",
    " 'school_km',\n",
    " 'power_transmission_line_km',\n",
    " 'water_km',\n",
    " 'green_part_1000',\n",
    " 'church_synagogue_km',\n",
    " 'deposits_rate',\n",
    " 'rent_price_4+room_bus',\n",
    " 'railroad_station_walk_km',\n",
    " 'office_km',\n",
    " 'kitch_sq',\n",
    " 'fitness_km',\n",
    " 'rel_kitch_sq',\n",
    " 'ttk_km',\n",
    " 'indust_part',\n",
    " 'cemetery_km',\n",
    " 'big_road1_km',\n",
    " 'stadium_km',\n",
    " 'trc_sqm_5000',\n",
    " 'railroad_station_avto_km',\n",
    " 'balance_trade_growth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importance=pd.read_csv('importance.csv')\n",
    "df_columns=list( set(importance_features+add_macro_cols+['rel_floor','rel_kitch_sq',\"ratio_life_sq_full_sq\",\"age_of_building\",'fist_floor','last_floor','year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain_all = xgb.DMatrix(X_train_all[df_columns], ylog_train_all, feature_names=df_columns)\n",
    "dtrain = xgb.DMatrix(X_train[df_columns], ylog_train, feature_names=df_columns)\n",
    "dval = xgb.DMatrix(X_val[df_columns], ylog_val, feature_names=df_columns)\n",
    "dtest = xgb.DMatrix(X_test[df_columns], feature_names=df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seeds=random.sample(range(2000), 20)\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 1.0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 0,'seed':123\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  2725750.65  std:  14004.4027147\n",
      "Wall time: 6min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res=[]\n",
    "# Uncomment to tune XGB `num_boost_rounds`\n",
    "for seed in seeds:\n",
    "    xgb_params['seed']=seed\n",
    "    partial_model = xgb.train(xgb_params, dtrain, num_boost_round=1000, evals=[(dval, 'val')],\n",
    "                           early_stopping_rounds=20, verbose_eval=False)\n",
    "\n",
    "    num_boost_round = partial_model.best_iteration\n",
    "    best_score = partial_model.best_score\n",
    "    res.append([num_boost_round,best_score,seed])\n",
    "r=np.array([el[1] for el in res])\n",
    "print('best score: ' ,r.mean(),' std: ',r.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('best score: ', 2725750.6499999999, ' std: ', 14004.402714726893)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=np.array([el[1] for el in res])\n",
    "'best score: ' ,r.mean(),' std: ',r.std()\n",
    "#('best score: ', 0.42012769999999994, ' std: ', 0.001197290027520486)\n",
    "\n",
    "#best score:  0.4210585  std:  0.000947589388923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importance=partial_model.get_fscore()\n",
    "#importance=pd.DataFrame(list(importance.items()),columns=['Feature','importance']).sort_values(by='importance',\n",
    "#                                                                                               ascending=False).reset_index(drop=True)\n",
    "\n",
    "#importance.to_csv('importance.csv',index=False)\n",
    "#df_columns= list(importance[importance.importance>=32].Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 60))\n",
    "bar=xgb.plot_importance(partial_model, max_num_features=50, height=0.5, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_pred=[]\n",
    "for el in res:\n",
    "    xgb_params['seed']=el[2]\n",
    "    model = xgb.train(xgb_params, dtrain_all, num_boost_round=el[0])\n",
    "    ylog_pred = model.predict(dtest)#*X_test.usdrub\n",
    "    y_pred =ylog_pred #(np.exp(ylog_pred) - 1)\n",
    "    final_pred.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_sub = pd.DataFrame({'id': id_test, 'price_doc': list(pd.DataFrame(final_pred).mean())})\n",
    "\n",
    "df_sub.to_csv(os.path.join(ml.project_save,'5.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_all shape is (30471, 323)\n",
      "X_train shape is (24377, 323)\n",
      "y_train shape is (24377,)\n",
      "X_val shape is (6094, 323)\n",
      "y_val shape is (6094,)\n",
      "X_test shape is (7662, 323)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy values\n",
    "\n",
    "num_train=df_train.shape[0]\n",
    "# Create a validation set, with last 20% of data\n",
    "num_val = int(num_train * 0.2)\n",
    "\n",
    "X_train_all = a[0]\n",
    "X_train = X_train_all[:num_train-num_val]\n",
    "X_val = X_train_all[num_train-num_val:num_train]\n",
    "#ylog_train_all =np.log1p( a[1])\n",
    "ylog_train_all=a[1]/a[0].usdrub\n",
    "#ylog_train_all =np.log1p( corrected_price)\n",
    "\n",
    "ylog_train = ylog_train_all[:-num_val]\n",
    "ylog_val = ylog_train_all[-num_val:]\n",
    "\n",
    "X_test = a[2]\n",
    "\n",
    "df_columns = a[0].columns\n",
    "\n",
    "print('X_train_all shape is', X_train_all.shape)\n",
    "print('X_train shape is', X_train.shape)\n",
    "print('y_train shape is', ylog_train.shape)\n",
    "print('X_val shape is', X_val.shape)\n",
    "print('y_val shape is', ylog_val.shape)\n",
    "print('X_test shape is', X_test.shape)\n",
    "\n",
    "dtrain_all = xgb.DMatrix(X_train_all[df_columns], ylog_train_all, feature_names=df_columns)\n",
    "dtrain = xgb.DMatrix(X_train[df_columns], ylog_train, feature_names=df_columns)\n",
    "dval = xgb.DMatrix(X_val[df_columns], ylog_val, feature_names=df_columns)\n",
    "dtest = xgb.DMatrix(X_test[df_columns], feature_names=df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  56416.7824218  std:  308.851153935\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res=[]\n",
    "# Uncomment to tune XGB `num_boost_rounds`\n",
    "for seed in seeds:\n",
    "    xgb_params['seed']=seed\n",
    "    partial_model = xgb.train(xgb_params, dtrain, num_boost_round=1000, evals=[(dval, 'val')],\n",
    "                           early_stopping_rounds=20, verbose_eval=False)\n",
    "\n",
    "    num_boost_round = partial_model.best_iteration\n",
    "    best_score = partial_model.best_score\n",
    "    res.append([num_boost_round,best_score,seed])\n",
    "r=np.array([el[1] for el in res])\n",
    "print('best score: ' ,r.mean(),' std: ',r.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_pred=[]\n",
    "for el in res:\n",
    "    xgb_params['seed']=el[2]\n",
    "    model = xgb.train(xgb_params, dtrain_all, num_boost_round=el[0])\n",
    "    ylog_pred = model.predict(dtest)*X_test.usdrub\n",
    "    y_pred =ylog_pred #(np.exp(ylog_pred) - 1)\n",
    "    final_pred.append(y_pred)\n",
    "    \n",
    "df_sub = pd.DataFrame({'id': id_test, 'price_doc': list(pd.DataFrame(final_pred).mean())})\n",
    "\n",
    "df_sub.to_csv(os.path.join(ml.project_save,'6.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usdmeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_all shape is (30471, 323)\n",
      "X_train shape is (24377, 323)\n",
      "y_train shape is (24377,)\n",
      "X_val shape is (6094, 323)\n",
      "y_val shape is (6094,)\n",
      "X_test shape is (7662, 323)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy values\n",
    "\n",
    "num_train=df_train.shape[0]\n",
    "# Create a validation set, with last 20% of data\n",
    "num_val = int(num_train * 0.2)\n",
    "\n",
    "X_train_all = a[0]\n",
    "X_train = X_train_all[:num_train-num_val]\n",
    "X_val = X_train_all[num_train-num_val:num_train]\n",
    "#ylog_train_all =np.log1p( a[1])\n",
    "ylog_train_all=a[1]/a[0].usdrub/a[0].full_sq\n",
    "#ylog_train_all =np.log1p( corrected_price)\n",
    "\n",
    "ylog_train = ylog_train_all[:-num_val]\n",
    "ylog_val = ylog_train_all[-num_val:]\n",
    "\n",
    "X_test = a[2]\n",
    "\n",
    "df_columns = a[0].columns\n",
    "\n",
    "print('X_train_all shape is', X_train_all.shape)\n",
    "print('X_train shape is', X_train.shape)\n",
    "print('y_train shape is', ylog_train.shape)\n",
    "print('X_val shape is', X_val.shape)\n",
    "print('y_val shape is', ylog_val.shape)\n",
    "print('X_test shape is', X_test.shape)\n",
    "\n",
    "dtrain_all = xgb.DMatrix(X_train_all[df_columns], ylog_train_all, feature_names=df_columns)\n",
    "dtrain = xgb.DMatrix(X_train[df_columns], ylog_train, feature_names=df_columns)\n",
    "dval = xgb.DMatrix(X_val[df_columns], ylog_val, feature_names=df_columns)\n",
    "dtest = xgb.DMatrix(X_test[df_columns], feature_names=df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  816.2432587  std:  1.80074722324\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res=[]\n",
    "# Uncomment to tune XGB `num_boost_rounds`\n",
    "for seed in seeds:\n",
    "    xgb_params['seed']=seed\n",
    "    partial_model = xgb.train(xgb_params, dtrain, num_boost_round=1000, evals=[(dval, 'val')],\n",
    "                           early_stopping_rounds=20, verbose_eval=False)\n",
    "\n",
    "    num_boost_round = partial_model.best_iteration\n",
    "    best_score = partial_model.best_score\n",
    "    res.append([num_boost_round,best_score,seed])\n",
    "r=np.array([el[1] for el in res])\n",
    "print('best score: ' ,r.mean(),' std: ',r.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_pred=[]\n",
    "for el in res:\n",
    "    xgb_params['seed']=el[2]\n",
    "    model = xgb.train(xgb_params, dtrain_all, num_boost_round=el[0])\n",
    "    ylog_pred = model.predict(dtest)*X_test.usdrub*X_test.full_sq\n",
    "    y_pred =ylog_pred #(np.exp(ylog_pred) - 1)\n",
    "    final_pred.append(y_pred)\n",
    "    \n",
    "df_sub = pd.DataFrame({'id': id_test, 'price_doc': list(pd.DataFrame(final_pred).mean())})\n",
    "\n",
    "df_sub.to_csv(os.path.join(ml.project_save,'7.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_all shape is (30471, 323)\n",
      "X_train shape is (24377, 323)\n",
      "y_train shape is (24377,)\n",
      "X_val shape is (6094, 323)\n",
      "y_val shape is (6094,)\n",
      "X_test shape is (7662, 323)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy values\n",
    "\n",
    "num_train=df_train.shape[0]\n",
    "# Create a validation set, with last 20% of data\n",
    "num_val = int(num_train * 0.2)\n",
    "\n",
    "X_train_all = a[0]\n",
    "X_train = X_train_all[:num_train-num_val]\n",
    "X_val = X_train_all[num_train-num_val:num_train]\n",
    "#ylog_train_all =np.log1p( a[1])\n",
    "ylog_train_all=a[1]/a[0].full_sq\n",
    "#ylog_train_all =np.log1p( corrected_price)\n",
    "\n",
    "ylog_train = ylog_train_all[:-num_val]\n",
    "ylog_val = ylog_train_all[-num_val:]\n",
    "\n",
    "X_test = a[2]\n",
    "\n",
    "df_columns = a[0].columns\n",
    "\n",
    "print('X_train_all shape is', X_train_all.shape)\n",
    "print('X_train shape is', X_train.shape)\n",
    "print('y_train shape is', ylog_train.shape)\n",
    "print('X_val shape is', X_val.shape)\n",
    "print('y_val shape is', ylog_val.shape)\n",
    "print('X_test shape is', X_test.shape)\n",
    "\n",
    "dtrain_all = xgb.DMatrix(X_train_all[df_columns], ylog_train_all, feature_names=df_columns)\n",
    "dtrain = xgb.DMatrix(X_train[df_columns], ylog_train, feature_names=df_columns)\n",
    "dval = xgb.DMatrix(X_val[df_columns], ylog_val, feature_names=df_columns)\n",
    "dtest = xgb.DMatrix(X_test[df_columns], feature_names=df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  40949.5316406  std:  100.616211754\n",
      "Wall time: 13min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res=[]\n",
    "# Uncomment to tune XGB `num_boost_rounds`\n",
    "for seed in seeds:\n",
    "    xgb_params['seed']=seed\n",
    "    partial_model = xgb.train(xgb_params, dtrain, num_boost_round=1000, evals=[(dval, 'val')],\n",
    "                           early_stopping_rounds=20, verbose_eval=False)\n",
    "\n",
    "    num_boost_round = partial_model.best_iteration\n",
    "    best_score = partial_model.best_score\n",
    "    res.append([num_boost_round,best_score,seed])\n",
    "r=np.array([el[1] for el in res])\n",
    "print('best score: ' ,r.mean(),' std: ',r.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_pred=[]\n",
    "for el in res:\n",
    "    xgb_params['seed']=el[2]\n",
    "    model = xgb.train(xgb_params, dtrain_all, num_boost_round=el[0])\n",
    "    ylog_pred = model.predict(dtest)*X_test.full_sq\n",
    "    y_pred =ylog_pred #(np.exp(ylog_pred) - 1)\n",
    "    final_pred.append(y_pred)\n",
    "    \n",
    "df_sub = pd.DataFrame({'id': id_test, 'price_doc': list(pd.DataFrame(final_pred).mean())})\n",
    "\n",
    "df_sub.to_csv(os.path.join(ml.project_save,'8.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
